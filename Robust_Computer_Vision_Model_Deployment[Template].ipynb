{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Robust Computer Vision Model Deployment[Template].ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Visionlogic-AI/CV_Model_Deployment/blob/master/Robust_Computer_Vision_Model_Deployment%5BTemplate%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVIAk5O6oeIW",
        "colab_type": "raw"
      },
      "source": [
        "In this notebook, we are goingt to go over the proper steps in order to deploy robust computer vision applications for a production environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvZDM5VFoeIh",
        "colab_type": "raw"
      },
      "source": [
        "SAVE MODEL FOR TENSORFLOW SERVING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WwAKZ-WoeIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#saving model\n",
        "model= create_maskrcnn_architecture_model(input_shape= INPUTE_SHAPE)\n",
        "model.load_weights('./model_weights/mask_rcnn_model_wwt.h5')\n",
        "export_path= './tf_saved_models'\n",
        "\n",
        "tf.saved_model.simple.save(\n",
        "keras.backend.get_sessions(),\n",
        "export_path,\n",
        "inputs={'input_image': model.inpout},\n",
        "outputs= {t.name:t for t in model.outputs})\n",
        "\n",
        "#view saved model(s)...if more than one model is being saved\n",
        "! tree --du -h ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFj6OWmmoeJF",
        "colab_type": "raw"
      },
      "source": [
        "Use tensorflow command line interface (CLI) tool to quikcly inspect the input and output specifications of our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB4m2eaZoeJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export PYTHONWARNINGS='ignore' && save _model_cli show --dir {'./tf_saved_model/'} --all"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWIqL63AoeJq",
        "colab_type": "raw"
      },
      "source": [
        "SERVE MODEL(S) WITH GPU INFERENCE\n",
        "*We will leverage Docker to setup our tensorflow serving system. \n",
        "\n",
        "Pulling TF Serivng GPU Image:\n",
        "First, we have to esure that docker is installed in our system (or in the coud).\n",
        "Once the proper docker file has been installed into our system, we can use the following code to pull the latest version of TF Serving on GPU's."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04ZK6NrGoeJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!docker pull tensorflow/serving:latest-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFEQvnHnoeJ8",
        "colab_type": "raw"
      },
      "source": [
        "Then we can check to ensure the image is present in our system using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G34As7TWoeJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!docker images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6RMyWKpoeKO",
        "colab_type": "raw"
      },
      "source": [
        "We are now ready to start serving models with TF Serving. We can do this by running the docker image that we have just downloaded.\n",
        "*This can be done directly in Jupyter Notebooks using the following code below. (In practice it is better to run it from the command terminal)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3HG6h3GoeKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash --bg\n",
        "docker run --runtime=nvidia -p 8501:8501 \\\n",
        "    --mount type= bind,source=/home/jupyter/tensorflow_serving/tf_saved_models,target=//home/jupyter/tensorflow_serving/tf_saved_models\\\n",
        "    --mount type=bind,source=/home/jupyter/tensorflow_serving/models.conf,target=/home/jupyter/tensorflow_serving/models/conf\\\n",
        "    -t tensorflow/serving:latest-gpu --model_config_file=/home/jupyter/tensorflow_servning/models.conf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knPsvX-6oeKj",
        "colab_type": "raw"
      },
      "source": [
        "#Then we can use the command \"in Docker\" to ensure the container is up and running"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk_gNXzwoeKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!docker ps -all # use this in docker!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24sbcWlIoeK0",
        "colab_type": "raw"
      },
      "source": [
        "Finally, we can chec the logs inside of Docker to verify that everything is working perfectly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUOa0jlLoeK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!docker logs 7d4b091ccefa | tail -n 15\n",
        "\n",
        "#this verifies the fact that TF Serving will be using the GPU on our system for inference!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2_Z8vQIoeLK",
        "colab_type": "text"
      },
      "source": [
        "MODEL WARMUP\n",
        "We can leverage our previously implemented code to warmup our model(s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzdkgBAqoeLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "warmup_data= np.load('serve_warmup_data.npy')\n",
        "warmup_model_serve(warmup_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-nShsGMoeLi",
        "colab_type": "raw"
      },
      "source": [
        "BECNHMARK MODEL SERVING REQUESTS:\n",
        "Here, we will take all of our test images and send a single request to check model serving time for inference using our GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Trk44Q7oeLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%time\n",
        "\n",
        "sample_test_data= test_images\n",
        "sample_test_labels= test_labels\n",
        "IMG_DIMS= (32,32) \n",
        "sample_test_data_processed = (np.array([resize_image_array(img,\n",
        "                                                          img_size_dims= IMG_DIMS)\n",
        "                                       for img in np.stack([sample_test_data]*3,\n",
        "                                                          axis= -1)])) / 255.\n",
        "data = json.dumps({'signature_name': 'serving_default',\n",
        "                  'instances': sample_test_data_processed.tolist()})\n",
        "\n",
        "HEADERS= {'content-type': 'application/json'}\n",
        "MODEL_API_URL= 'http:localhost:8501/v1/models/maskrcnn_model_serving/versions/:predict'\n",
        "\n",
        "json_response= requests.post(MODEL_API_URL, data=data, headers= HEADERS)\n",
        "predictions= json.loads(json_response.text)['predictions']\n",
        "predictions= np.argmax(np.array(predictions), axis=1)\n",
        "predictions_labels = [class_names[p] for p in predictions]\n",
        "len(prediction_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwvlPQRSoeLz",
        "colab_type": "raw"
      },
      "source": [
        "Lets now perform REAL BECNHMARK TESTING\n",
        "Now lets, consider x amount of \"seperate\" requests for inference of a single image to be detected each time.\n",
        "(This will show us how much time it would tae for tf serving to serve these x amount of requests)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz8lRLyXoeMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create serving function\n",
        "def predict_maskrcnn_model_serving(img, img_dims=(32,32), label_map=class_names):\n",
        "    #create sample image to prcoess\n",
        "    sample_img_processed= (np.array([resize_image_array(img,\n",
        "                                                       img_size_dims=img_dims)\n",
        "                                    for img in np.stack([[img]]*3,\n",
        "                                                       axis= -1)]))/ 255.\n",
        "    \n",
        "    data= json.dumps({'signature_name': 'serving_default',\n",
        "                     'instances': sample_img_processed.tolist()})\n",
        "    \n",
        "    HEADERS= {'content-type': 'application/json'}\n",
        "    \n",
        "    MODEL_API_URL= 'http://localhost:8501/v1/model/maskrcnn_model_serving/:predict'\n",
        "    json_response= requests.post(MODEL_API_URL, data=data, headers= HEADERS)\n",
        "    prediction= json.loads(json_response_text)['predictions']\n",
        "    prediction= np.argmax(np.array(prediction), axis=1)[0]\n",
        "    return label_map[prediction]\n",
        "\n",
        "#bench for x amount of request\n",
        "%%time\n",
        "\n",
        "pred_labels= []\n",
        "for img in tqdm(test_images):\n",
        "    pred_label= predict_maskrcnn_model_serving(img)\n",
        "    pred_labels.append(img)\n",
        "len(pred_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k7YtBF_oeMJ",
        "colab_type": "raw"
      },
      "source": [
        "Now lets try an interesting comparison in order to see how long it takes using the regular api from Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UHOpfgioeMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create keras serving function\n",
        "INPUT_SHAPE_RN= (32,32,3)\n",
        "model= create_maskrcnn_architecture_model(input_shape=INPUT_SHAPE_RN)\n",
        "model.load_weight('./model_weights/maskrcnn_model_wt.h5')  #make sure this is the name of your exact model\n",
        "\n",
        "def predict_maskrcnn_model_regular(img, img_dims=(32,32), label_map= class_names):\n",
        "    #create sample image processing\n",
        "    sample_img_prcoessed= (np.array([resize_image_array(img,\n",
        "                                                       img_size_dims= img_dims)\n",
        "                                    for img in np.stack([[img]]*3,\n",
        "                                                       axis= -1)]))/ 255.\n",
        "    prediction= model.predict(sample_img_processed)\n",
        "    prediction = np.argmax(np.array(prediction), axis=1)[0]\n",
        "    return label_map[prediction]\n",
        "\n",
        "#benchmrk x number of request\n",
        "%%time\n",
        "\n",
        "pred_labels= []\n",
        "for img in tqdm(test_images):\n",
        "    pred_lael= predict_maskrcnn_model_regular(img)\n",
        "    pred_labels.append(img)\n",
        "len(pred_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_P2CB4IoeMp",
        "colab_type": "raw"
      },
      "source": [
        "LEVERAGING FLASK TO BUILD A WEB SERVICE/API ON TOP OF TF SERVING \n",
        "This will allow:\n",
        "    - allow us to accept real world images \n",
        "    - conduct the proper pre-precesing\n",
        "    - call TF Serving\n",
        "    - post process rhe response\n",
        "    - and then send the final json response to the end user\n",
        "   \n",
        "Do Note: We can even dockerize and deploy the Flask ApI on Kubernetes or use a WSGI server like Gunicorn to scale and improve performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKVSsjS7oeMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create API with Flask\n",
        "#load dependencies\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import numpy as np\n",
        "import requests\n",
        "import base64\n",
        "import json\n",
        "from io import BytesIO\n",
        "from flask import Flask, requests, jsonify\n",
        "from flask_cors import CORS\n",
        "import tensorflow as tf\n",
        "from tensorflow import Keras\n",
        "import cv2\n",
        "\n",
        "#TF Serving Assets\n",
        "HEADERS= {'content-type': 'application/json'}\n",
        "MODEL_API_URL= 'http:localhost:8501/v1/models/maskrcnn_model_serving/:predict'\n",
        "CLASS_NAMES= ['list_class_names_here']  #here is where we list all of the class names for our model\n",
        "\n",
        "\n",
        "#Instantiate the Flask App\n",
        "app= Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "#Image resizing utils\n",
        "def resize_image_array(img, img_size_dims):\n",
        "    img= cv2.resize(img, dsize=img_size_dims,\n",
        "                   interpolation= cv2.INTER_CUBIC)\n",
        "    IMG= np.array(img, dtype=np.float32)\n",
        "    return img\n",
        "\n",
        "#model warmup function\n",
        "def warmup_model_serve(warmup_data, img_dims=(32,32)):\n",
        "    warmup_data_processed= (np.array([resize_image_array(img,\n",
        "                                                       img_size_dims= img_dims)\n",
        "                                    for img in np.stack([warmup_data]*3,\n",
        "                                                       axis= -1)])) / 255.\n",
        "    \n",
        "    data= json.dumps({'signature_name': 'serving_default',\n",
        "                     'instances': warmup_data_processed.tolist()})\n",
        "    \n",
        "    json_response= requests.post(MODEL_API_URL, data= data, headers= HEADERS)\n",
        "    predictions= json.loads(json_response.text)['predictions']\n",
        "    print('Model warmup complete ') #log this in actual production\n",
        "    \n",
        "#TF lazy loads so we warmup model with sample data\n",
        "#this runs as soon as we setup our web service to run\n",
        "warmup_data= np.load('serve_warmup_data.npy')\n",
        "warmup_model_serve(warmup_data)\n",
        "\n",
        "#Livesness Test \n",
        "@app.route('/maskrcnn_detection/api/v1/liveness', methods=['GET', 'POST'])  #make sure this mathes your model name\n",
        "def liveness():\n",
        "    return 'API Live!'\n",
        "\n",
        "#Model inference endpoint\n",
        "@app.route('maskrcnn_detection/api/v1/model_predict', methods= ['POST'])\n",
        "def maskrcnn_detection_model():\n",
        "    img= np.array([keras.preprocessing.image.img_to_array(\n",
        "    keras.preprocessing.image.load_img(BytesIO(base64.b64decode(request.form['b64_img'])),\n",
        "                                      target_size=(32,32))) / 255.])\n",
        "    data= json.dumps({'signature_name': 'serving_default',\n",
        "                     'instances': img.tolist()})\n",
        "    json_response= requests.post(MODEL_API_URL, data=data, headers= HEADERS)\n",
        "    prediction= json.loads(json_response.text)['predictions']\n",
        "    prediction= np.argmax(np.asarray(predictions), axis=1)[0]\n",
        "    prediction= CLASS_NAMES[prediction]\n",
        "    \n",
        "    return jsonify({'vision_prediction': prediction})  #this name can be anything you choose\n",
        "\n",
        "#running REST interface, port=5000 for direct test\n",
        "#use debug= True when debugging\n",
        "if __name__== '__main__':\n",
        "    app.run(debug= False, host= '0.0.0.0', port=5000)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_GPkrU-oeM7",
        "colab_type": "raw"
      },
      "source": [
        "We store this file as app.py in our server which forms the base of our api"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlC4X_hNoeNA",
        "colab_type": "raw"
      },
      "source": [
        "STORE DOCKER CONTAINER FOR TF SERVING:\n",
        "Check and restart the docker container for TF Serving if not's already up and running"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDN8zObmoeND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run this code in docker command\n",
        "!docker start 7d4b091ccefa\n",
        "!docker ps -all"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5n8P1jToeNX",
        "colab_type": "raw"
      },
      "source": [
        "START OUR COMPUTER VISION OBJECT DETECTION/RECOGNITION MODEL\n",
        "*Now we need to start our web service. \n",
        "In production, it is vital that we do not use the default web server provided by flask but a better production ready WSGI server, like Gunicorn.\n",
        "We start our web service using the following command \"from terminal\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTrM-biWoeNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/home/jupyter/tensorflow_serving$ gunicorn -b 0.0.0.0:5000 app:app -w 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3K5IjLuoeNn",
        "colab_type": "raw"
      },
      "source": [
        "We leverage multiple workers to serve more requests as needed. \n",
        "Lets now check if our API is live using the \"liveness test endpoint\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPcTkS0loeNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r= requests.get('http://0.0.0.0:5000/vision_prediction/api/v1/liveness')\n",
        "r.status_code, r.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAYwqAqtoeN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SERVE SAMPLE OBJECT DETECTION/RECOGNITION WITH WEB SERVICE\n",
        "plt.imshow(cv2.cvtColor(cv2.imread('image.jpg'),\n",
        "                       cv2.COLOR_BGR2RRGB))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYVx3xsMoeOG",
        "colab_type": "raw"
      },
      "source": [
        "Once the sample image is what we are looking for, we can then leverage our API to serve the model prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlL-vCt9oeOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import base64\n",
        "import requests\n",
        "\n",
        "with open('image.jpg', 'rb') as imageFile:\n",
        "    img_b64enc= base64.b64encoder(imageFile.read())\n",
        "    data= {'b64_img': img_b64enc}\n",
        "\n",
        "API_URL= 'http://0.0.0.0:5000/vision_prediction/api/v1/model_predict'  #make sure your model matches from what you created\n",
        "#sending post request and saving response as response object\n",
        "r = requests.post(url= API_URL, data=data)\n",
        "r.json()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAi6m6ayoeOV",
        "colab_type": "raw"
      },
      "source": [
        "Once the  prediction we are looking for is accurate, we then need to benchmark our web service\n",
        "Lets check out how much time it takes to process 10000 requests now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDKFnOK9oeOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "responses= []\n",
        "for i in tqdm(range(10000)):\n",
        "    API_URL= 'http://0.0.0.0:5000/vision_prediction/api/v1/model_predict'  #check and solidify your exact name\n",
        "    #sending post request and saving response as response object\n",
        "    r= request.append(r.json())\n",
        "    \n",
        "len(responses)\n",
        "print('Inference time per image: {} ms'.format((326/10000) *1000))TYUIOP[] \\"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}